{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34372491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import toml\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de5ff210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEF CONSTATS\n",
    "\n",
    "LABELS_PATH = \"/home/kuba/Desktop/smoking_labels_export_2025-07-01_16.json\"\n",
    "NUM_PARTICIPATES = 10\n",
    "TRAIN_PERCENT = 0.6\n",
    "DEV_PERCENT = 0.2\n",
    "TEST_PERCENT = (1 - TRAIN_PERCENT - DEV_PERCENT)\n",
    "RANDOM_SEED = 70\n",
    "USE_GYRO = False\n",
    "LABEL = 'puff'\n",
    "LABEL_VALUE = 1 #what to place in the y vector \n",
    "RESAMPLE = False\n",
    "PERCENT_OF_NEGATIVE_WINDOWS_TO_SAMPLE = 1.0 #from all windows that don't contain a label what percent to sample\n",
    "THRESHOLD_FOR_GAP = 30 #min\n",
    "SAVE_DIR = '/home/kuba/projects/puff/test/data'\n",
    "WINDOW_SIZE = 512\n",
    "WINDOW_OVERLAP = WINDOW_SIZE // 2\n",
    "\n",
    "TRAIN_IDS = []\n",
    "DEV_IDS = []\n",
    "TEST_IDS = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daa3fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_config():\n",
    "    \"\"\"save experiment configuration to toml file\"\"\"\n",
    "    config = {\n",
    "        \"paths\": {\n",
    "            \"labels_path\": LABELS_PATH,\n",
    "            \"save_dir\": SAVE_DIR,\n",
    "        },\n",
    "        \"experiment\": {\n",
    "            \"label\": LABEL,\n",
    "            \"resample\": RESAMPLE,\n",
    "            \"random_seed\": RANDOM_SEED,\n",
    "            \"window_size\": WINDOW_SIZE,\n",
    "            \"window_overlap\": WINDOW_OVERLAP,\n",
    "            \"percent_negative_windows\": PERCENT_OF_NEGATIVE_WINDOWS_TO_SAMPLE,\n",
    "            \"threshold_gap_minutes\": THRESHOLD_FOR_GAP,\n",
    "            \"use_gyro\": USE_GYRO\n",
    "        },\n",
    "        \"split\": {\n",
    "            \"train_percent\": TRAIN_PERCENT,\n",
    "            \"dev_percent\": DEV_PERCENT,\n",
    "            \"test_percent\": TEST_PERCENT,\n",
    "            \"num_participants\": NUM_PARTICIPATES,\n",
    "        },\n",
    "        \"splits\": {\n",
    "            \"train_ids\": TRAIN_IDS.tolist() if isinstance(TRAIN_IDS, np.ndarray) else TRAIN_IDS,\n",
    "            \"dev_ids\": DEV_IDS.tolist() if isinstance(DEV_IDS, np.ndarray) else DEV_IDS,\n",
    "            \"test_ids\": TEST_IDS.tolist() if isinstance(TEST_IDS, np.ndarray) else TEST_IDS\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    with open(os.path.join(SAVE_DIR, 'config.toml'), \"w\") as f:\n",
    "        toml.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92b965f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_splits():\n",
    "    \"\"\"makesure  dataset splits add up to 1.0\"\"\"\n",
    "    if abs(TRAIN_PERCENT + DEV_PERCENT + TEST_PERCENT - 1.0) > 1e-6:\n",
    "        raise ValueError(f\"dataset percents must add up to 1, not {TRAIN_PERCENT + DEV_PERCENT + TEST_PERCENT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b000c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_participant_splits():\n",
    "    \"\"\"create random train/dev/test splits of participants\"\"\"\n",
    "    global TRAIN_IDS, DEV_IDS, TEST_IDS\n",
    "    \n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    random_perm = np.random.permutation(NUM_PARTICIPATES)\n",
    "    train_size = int(len(random_perm) * TRAIN_PERCENT)\n",
    "    dev_size = int(len(random_perm) * DEV_PERCENT)\n",
    "\n",
    "    TRAIN_IDS = random_perm[:train_size]\n",
    "    DEV_IDS = random_perm[train_size:train_size + dev_size]\n",
    "    TEST_IDS = random_perm[train_size + dev_size:]\n",
    "    \n",
    "    print(f'TRAIN ids: {TRAIN_IDS}')\n",
    "    print(f'DEV ids: {DEV_IDS}')\n",
    "    print(f'TEST ids: {TEST_IDS}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fddc0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(df) :\n",
    "    \"\"\"resample dataframe to consistent sampling rate\"\"\"\n",
    "    print(\"RESAMPLE has not been added yet so you need to impliment the function\")\n",
    "    raise RuntimeError(\"The resample function has not been implimented \")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c1ef76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_gaps(df):\n",
    "    \"\"\"split dataframe on time gaps larger than threshold\"\"\"\n",
    "    gap_threshold_ns = THRESHOLD_FOR_GAP * 60 * 1_000_000_000\n",
    "    df = df.sort_values('ns_since_reboot').reset_index(drop=True)\n",
    "    time_diffs = df['ns_since_reboot'].diff()\n",
    "    gap_indices = time_diffs[time_diffs > gap_threshold_ns].index\n",
    "    \n",
    "    if len(gap_indices) == 0:\n",
    "        return [df]\n",
    "    \n",
    "    # split into segments\n",
    "    segments = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    for gap_idx in gap_indices:\n",
    "        if start_idx < gap_idx:\n",
    "            segment = df.iloc[start_idx:gap_idx].copy()\n",
    "            if not segment.empty:\n",
    "                segments.append(segment)\n",
    "        start_idx = gap_idx\n",
    "    \n",
    "    # add final segment\n",
    "    if start_idx < len(df):\n",
    "        final_segment = df.iloc[start_idx:].copy()\n",
    "        if not final_segment.empty:\n",
    "            segments.append(final_segment)\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1263eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(session, project_path: str) -> pd.DataFrame:\n",
    "    \"\"\"combine accelerometer and gyroscope data for a session\"\"\"\n",
    "    data_path = os.path.join(project_path, session['session_name'])\n",
    "    \n",
    "    try:\n",
    "        accl = pd.read_csv(os.path.join(data_path, 'accelerometer_data.csv'))\n",
    "        if USE_GYRO:\n",
    "            gyro = pd.read_csv(os.path.join(data_path, 'gyroscope_data.csv'))\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Warning: Could not find data files for session {session['session_name']}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # rename columns to avoid conflicts\n",
    "    accl = accl.rename(columns={\"x\": \"x_acc\", \"y\": \"y_acc\", \"z\": \"z_acc\"})\n",
    "    \n",
    "    # ensure data types are correct\n",
    "    for col in ['ns_since_reboot', 'x_acc', 'y_acc', 'z_acc']:\n",
    "        accl[col] = accl[col].astype(float)\n",
    "    \n",
    "    if USE_GYRO:\n",
    "        gyro = gyro.rename(columns={\"x\": \"x_gyro\", \"y\": \"y_gyro\", \"z\": \"z_gyro\"})\n",
    "        for col in ['ns_since_reboot', 'x_gyro', 'y_gyro', 'z_gyro']:\n",
    "            gyro[col] = gyro[col].astype(float)\n",
    "        \n",
    "        # combine accelerometer and gyroscope data\n",
    "        combined = pd.merge(accl, gyro, on='ns_since_reboot', how='inner')\n",
    "        column_order = ['ns_since_reboot', 'x_acc', 'y_acc', 'z_acc', 'x_gyro', 'y_gyro', 'z_gyro']\n",
    "    else:\n",
    "        # use only accelerometer data\n",
    "        combined = accl\n",
    "        column_order = ['ns_since_reboot', 'x_acc', 'y_acc', 'z_acc']\n",
    "    \n",
    "    # reorder columns\n",
    "    combined = combined[column_order]\n",
    "    \n",
    "    return combined.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfb7d9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_labels_to_df(df, session) -> pd.DataFrame:\n",
    "    \"\"\"add labels to dataframe based on bout annotations\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    bout_starts = []\n",
    "    bout_ends = []\n",
    "\n",
    "    for bout in session.get('bouts', []):\n",
    "        if bout.get('label') == LABEL:\n",
    "            bout_starts.append(bout['start_time'])\n",
    "            bout_ends.append(bout['end_time'])\n",
    "\n",
    "    df['label'] = 0\n",
    "\n",
    "    for start, stop in zip(bout_starts, bout_ends):\n",
    "        mask = (df['ns_since_reboot'] >= start) & (df['ns_since_reboot'] <= stop)\n",
    "        df.loc[mask, 'label'] = LABEL_VALUE\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69fd2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(df):\n",
    "    \"\"\"create sliding windows from dataframe\"\"\"\n",
    "    if len(df) < WINDOW_SIZE:\n",
    "        print(f\"Warning: DataFrame too small ({len(df)} < {WINDOW_SIZE}), skipping\")\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    if USE_GYRO:\n",
    "        feature_cols = ['x_acc', 'y_acc', 'z_acc', 'x_gyro', 'y_gyro', 'z_gyro']\n",
    "    else:\n",
    "        feature_cols = ['x_acc', 'y_acc', 'z_acc']\n",
    "        \n",
    "    X_data = df[feature_cols].values\n",
    "    y_data = df['label'].values\n",
    "    \n",
    "    windows_X = []\n",
    "    windows_y = []\n",
    "    \n",
    "    for i in range(0, len(df) - WINDOW_SIZE + 1, WINDOW_OVERLAP):\n",
    "        window_X = X_data[i:i + WINDOW_SIZE]\n",
    "        window_y = y_data[i:i + WINDOW_SIZE]\n",
    "        \n",
    "        windows_X.append(window_X)\n",
    "        windows_y.append(window_y)\n",
    "    \n",
    "    return np.array(windows_X), np.array(windows_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36314f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_negative_windows(X, y) :\n",
    "    \"\"\"sample negative windows based on configured percentage\"\"\"\n",
    "    if PERCENT_OF_NEGATIVE_WINDOWS_TO_SAMPLE >= 1.0:\n",
    "        return X, y\n",
    "    \n",
    "    # find windows with and without labels\n",
    "    has_label = np.any(y > 0, axis=1)\n",
    "    print(f\"has labels shape: {has_label.shape}\")\n",
    "    positive_indices = np.where(has_label)[0]\n",
    "    negative_indices = np.where(~has_label)[0]\n",
    "    \n",
    "    # sample negative windows\n",
    "    num_negative_to_keep = int(len(negative_indices) * PERCENT_OF_NEGATIVE_WINDOWS_TO_SAMPLE)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    sampled_negative_indices = np.random.choice(negative_indices, size=num_negative_to_keep, replace=False)\n",
    "    \n",
    "    # combine positive and sampled negative windows\n",
    "    keep_indices = np.concatenate([positive_indices, sampled_negative_indices])\n",
    "    keep_indices = np.sort(keep_indices)\n",
    "    \n",
    "    return X[keep_indices], y[keep_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8a905b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_session(session, project_path) :\n",
    "    \"\"\"process a single session and return windowed data\"\"\"\n",
    "    df = combine(session, project_path)\n",
    "    \n",
    "    if df.empty:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # check for gaps and split if necessary\n",
    "    segments = check_for_gaps(df)\n",
    "    \n",
    "    all_windows_X = []\n",
    "    all_windows_y = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        if RESAMPLE:\n",
    "            segment = resample(segment)\n",
    "        \n",
    "        segment = apply_labels_to_df(segment, session)\n",
    "        windows_X, windows_y = create_windows(segment)\n",
    "        \n",
    "        if len(windows_X) > 0:\n",
    "            all_windows_X.append(windows_X)\n",
    "            all_windows_y.append(windows_y)\n",
    "    \n",
    "    if not all_windows_X:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # concatenate all segments\n",
    "    combined_X = np.concatenate(all_windows_X, axis=0)\n",
    "    combined_y = np.concatenate(all_windows_y, axis=0)\n",
    "    \n",
    "    return combined_X, combined_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32e3a84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_participant(participant) :\n",
    "    \"\"\"process all sessions for a participant\"\"\"\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    \n",
    "    for session in participant.get('sessions', []):\n",
    "        X, y = process_session(session, participant['project_path'])\n",
    "        \n",
    "        if len(X) > 0:\n",
    "            all_X.append(X)\n",
    "            all_y.append(y)\n",
    "    \n",
    "    if not all_X:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # concatenate all sessions\n",
    "    participant_X = np.concatenate(all_X, axis=0)\n",
    "    participant_y = np.concatenate(all_y, axis=0)\n",
    "    \n",
    "    return participant_X, participant_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b36f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(ids, labels_data):\n",
    "    \"\"\"create dataset from participant ids\"\"\"\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    \n",
    "    for project in labels_data['projects']:\n",
    "        participant_id = project['participant']['participant_id']\n",
    "        \n",
    "        if participant_id not in ids:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing participant {participant_id}\")\n",
    "        X, y = process_participant(project)\n",
    "        \n",
    "        if len(X) > 0:\n",
    "            all_X.append(X)\n",
    "            all_y.append(y)\n",
    "    \n",
    "    if not all_X:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # concatenate all participants\n",
    "    dataset_X = np.concatenate(all_X, axis=0)\n",
    "    dataset_y = np.concatenate(all_y, axis=0)\n",
    "    \n",
    "    # filter negative windows\n",
    "    dataset_X, dataset_y = filter_negative_windows(dataset_X, dataset_y)\n",
    "    \n",
    "    # shuffle the dataset\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    indices = np.random.permutation(len(dataset_X))\n",
    "    dataset_X = dataset_X[indices]\n",
    "    dataset_y = dataset_y[indices]\n",
    "    \n",
    "    print(f\"Dataset created with {len(dataset_X)} windows\")\n",
    "    \n",
    "    return dataset_X, dataset_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cabfc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(X: np.ndarray, y: np.ndarray, name: str):\n",
    "    \"\"\"save X and y tensors in a .pt file with the name as name.pt\"\"\"\n",
    "    if len(X) == 0:\n",
    "        print(f\"Warning: No data to save for {name}\")\n",
    "        return\n",
    "    \n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    # transpose X to have shape (batch_size, features, time_steps)\n",
    "    X_tensor = X_tensor.transpose(1, 2)\n",
    "    \n",
    "    save_path = os.path.join(SAVE_DIR, f\"{name}.pt\")\n",
    "    torch.save((X_tensor, y_tensor), save_path)\n",
    "    print(f\"Saved {name} dataset with shape X: {X_tensor.shape}, y: {y_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ccd87281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN ids: [5 8 1 9 7 3]\n",
      "DEV ids: [4 0]\n",
      "TEST ids: [2 6]\n",
      "Creating training dataset...\n",
      "Processing participant 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: DataFrame too small (501 < 512), skipping\n",
      "Warning: DataFrame too small (501 < 512), skipping\n",
      "Processing participant 1\n",
      "Warning: DataFrame too small (501 < 512), skipping\n",
      "Warning: DataFrame too small (502 < 512), skipping\n",
      "Warning: DataFrame too small (500 < 512), skipping\n",
      "Warning: DataFrame too small (502 < 512), skipping\n",
      "Warning: DataFrame too small (500 < 512), skipping\n",
      "Warning: DataFrame too small (501 < 512), skipping\n",
      "Warning: DataFrame too small (500 < 512), skipping\n",
      "Warning: DataFrame too small (502 < 512), skipping\n",
      "Warning: DataFrame too small (501 < 512), skipping\n",
      "Warning: DataFrame too small (499 < 512), skipping\n",
      "Dataset created with 87282 windows\n",
      "Saved trainhh dataset with shape X: torch.Size([87282, 3, 512]), y: torch.Size([87282, 512])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"main execution function\"\"\"\n",
    "# validate configuration\n",
    "validate_splits()\n",
    "\n",
    "# create participant splits\n",
    "create_participant_splits()\n",
    "\n",
    "# save configuration\n",
    "save_config()\n",
    "\n",
    "# load labels data\n",
    "with open(LABELS_PATH, 'r') as f:\n",
    "    labels_data = json.load(f)\n",
    "\n",
    "# create and save datasets\n",
    "print(\"Creating training dataset...\")\n",
    "train_X, train_y = make_dataset(TRAIN_IDS, labels_data)\n",
    "save_dataset(train_X, train_y, \"trainhh\")\n",
    "\n",
    "# print(\"Creating development dataset...\")\n",
    "# dev_X, dev_y = make_dataset(DEV_IDS, labels_data)\n",
    "# save_dataset(dev_X, dev_y, \"dev\")\n",
    "\n",
    "# print(\"Creating test dataset...\")\n",
    "# test_X, test_y = make_dataset(TEST_IDS, labels_data)\n",
    "# save_dataset(test_X, test_y, \"test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
