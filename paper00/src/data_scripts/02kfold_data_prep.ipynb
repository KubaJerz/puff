{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd5f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import toml\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eeea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL CONSTANTS - K-FOLD SPECIFIC\n",
    "PARTICIPANT_IDS = [1, 3, 6, 7, 10]  # List of participant IDs to include\n",
    "NUM_FOLDS = 5  # Number of folds for cross-validation\n",
    "KFOLD_BASE_DIR = '/home/kuba/Desktop/k-fold'  # Base directory containing fold subdirectories\n",
    "LABELS_PATH = \"/home/kuba/projects/puff/paper00/experiments/01/data/smoking_labels_export_2025-07-15_22.json\"  # Path to labels JSON file\n",
    "\n",
    "\n",
    "#########################################################\n",
    "#-------------------------------------------------------#\n",
    "#########################################################\n",
    "\n",
    "TRAIN_PERCENT = 0.6  # Percentage for training set (applied to non-test participants)\n",
    "DEV_PERCENT = (1-TRAIN_PERCENT)\n",
    "RANDOM_SEED = 70\n",
    "USE_GYRO = False\n",
    "LABEL = ['puff', 'puffs']\n",
    "LABEL_VALUE = 1\n",
    "RESAMPLE = False\n",
    "PERCENT_OF_NEGATIVE_WINDOWS_TO_SAMPLE = 0.5\n",
    "THRESHOLD_FOR_GAP = 30  # minutes\n",
    "WINDOW_SIZE = 256\n",
    "STEP_SIZE = 256  # Same as WINDOW_SIZE for non-overlapping windows\n",
    "\n",
    "# Global variables for current fold processing\n",
    "CURRENT_FOLD = 0\n",
    "CURRENT_SAVE_DIR = \"\"\n",
    "TRAIN_IDS = []\n",
    "DEV_IDS = []\n",
    "TEST_IDS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f8cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_kfold_structure():\n",
    "    \"\"\"Check that k-fold directory structure is correct\"\"\"\n",
    "    if not os.path.exists(KFOLD_BASE_DIR):\n",
    "        raise FileNotFoundError(f\"K-fold base directory does not exist: {KFOLD_BASE_DIR}\")\n",
    "    \n",
    "    # Check for exactly NUM_FOLDS subdirectories\n",
    "    expected_dirs = [f\"fold-{i}\" for i in range(NUM_FOLDS)]\n",
    "    existing_dirs = [d for d in os.listdir(KFOLD_BASE_DIR) \n",
    "                    if os.path.isdir(os.path.join(KFOLD_BASE_DIR, d))]\n",
    "    \n",
    "    missing_dirs = set(expected_dirs) - set(existing_dirs)\n",
    "    extra_dirs = set(existing_dirs) - set(expected_dirs)\n",
    "    \n",
    "    if missing_dirs:\n",
    "        raise FileNotFoundError(f\"Missing fold directories: {sorted(missing_dirs)}\")\n",
    "    \n",
    "    if extra_dirs:\n",
    "        print(f\"Warning: Found unexpected directories: {sorted(extra_dirs)}\")\n",
    "    \n",
    "    print(f\"Validated k-fold structure with {NUM_FOLDS} folds\")\n",
    "\n",
    "def validate_parameters():\n",
    "    \"\"\"Validate k-fold parameters\"\"\"\n",
    "    if not PARTICIPANT_IDS:\n",
    "        raise ValueError(\"PARTICIPANT_IDS cannot be empty\")\n",
    "    \n",
    "    if TRAIN_PERCENT + DEV_PERCENT != 1.0:\n",
    "        raise ValueError(f\"TRAIN_PERCENT + DEV_PERCENT must be == 1.0, got {TRAIN_PERCENT + DEV_PERCENT}\")\n",
    "    \n",
    "    if NUM_FOLDS > len(PARTICIPANT_IDS):\n",
    "        print(f\"Warning: NUM_FOLDS ({NUM_FOLDS}) > participants ({len(PARTICIPANT_IDS)}), \"\n",
    "              f\"this will create leave-one-out style validation\")\n",
    "    \n",
    "    if not os.path.exists(LABELS_PATH):\n",
    "        raise FileNotFoundError(f\"Labels file does not exist: {LABELS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02f5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kfold_splits():\n",
    "    \"\"\"Generate all k-fold participant splits\"\"\"\n",
    "    participants_per_fold = len(PARTICIPANT_IDS) // NUM_FOLDS\n",
    "    remainder = len(PARTICIPANT_IDS) % NUM_FOLDS\n",
    "    \n",
    "    splits = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    for fold_k in range(NUM_FOLDS):\n",
    "        # Calculate test set size for this fold\n",
    "        if fold_k < remainder:\n",
    "            test_size = participants_per_fold + 1\n",
    "        else:\n",
    "            test_size = participants_per_fold\n",
    "        \n",
    "        # Get test participants for this fold\n",
    "        test_ids = PARTICIPANT_IDS[start_idx:start_idx + test_size]\n",
    "        \n",
    "        # Get remaining participants for train/dev split\n",
    "        remaining_ids = [p for p in PARTICIPANT_IDS if p not in test_ids]\n",
    "        \n",
    "        # Split remaining participants into train/dev\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        random_perm = np.random.permutation(remaining_ids)\n",
    "        train_size = int(len(random_perm) * TRAIN_PERCENT)\n",
    "        dev_size = int(len(random_perm) - train_size)\n",
    "        \n",
    "        train_ids = random_perm[:train_size].tolist()\n",
    "        dev_ids = random_perm[train_size:train_size + dev_size].tolist()\n",
    "        \n",
    "        splits.append({\n",
    "            'fold': fold_k,\n",
    "            'train_ids': train_ids,\n",
    "            'dev_ids': dev_ids,\n",
    "            'test_ids': test_ids\n",
    "        })\n",
    "        \n",
    "        start_idx += test_size\n",
    "    \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0086930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_config(fold_num, train_ids, dev_ids, test_ids):\n",
    "    \"\"\"Save experiment configuration to toml file with k-fold specific parameters\"\"\"\n",
    "    config = {\n",
    "        \"kfold\": {\n",
    "            \"fold_number\": fold_num,\n",
    "            \"total_folds\": NUM_FOLDS,\n",
    "            \"participant_ids\": PARTICIPANT_IDS\n",
    "        },\n",
    "        \"splits\": {\n",
    "            \"train_ids\": train_ids,\n",
    "            \"dev_ids\": dev_ids,\n",
    "            \"test_ids\": test_ids\n",
    "        },\n",
    "        \"paths\": {\n",
    "            \"labels_path\": LABELS_PATH,\n",
    "            \"kfold_base_dir\": KFOLD_BASE_DIR,\n",
    "            \"save_dir\": CURRENT_SAVE_DIR\n",
    "        },\n",
    "        \"experiment\": {\n",
    "            \"label\": LABEL,\n",
    "            \"label_value\": LABEL_VALUE,\n",
    "            \"resample\": RESAMPLE,\n",
    "            \"random_seed\": RANDOM_SEED,\n",
    "            \"window_size\": WINDOW_SIZE,\n",
    "            \"step_size\": STEP_SIZE,\n",
    "            \"percent_negative_windows\": PERCENT_OF_NEGATIVE_WINDOWS_TO_SAMPLE,\n",
    "            \"threshold_gap_minutes\": THRESHOLD_FOR_GAP,\n",
    "            \"use_gyro\": USE_GYRO\n",
    "        },\n",
    "        \"split\": {\n",
    "            \"train_percent\": TRAIN_PERCENT,\n",
    "            \"dev_percent\": DEV_PERCENT,\n",
    "            \"test_percent\": 1.0 - TRAIN_PERCENT - DEV_PERCENT,\n",
    "            \"num_participants\": len(PARTICIPANT_IDS)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    os.makedirs(CURRENT_SAVE_DIR, exist_ok=True)\n",
    "    with open(os.path.join(CURRENT_SAVE_DIR, 'data_config.toml'), \"w\") as f:\n",
    "        toml.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_splits():\n",
    "    \"\"\"Make sure dataset splits add up to 1.0\"\"\"\n",
    "    test_percent = 1.0 - TRAIN_PERCENT - DEV_PERCENT\n",
    "    if abs(TRAIN_PERCENT + DEV_PERCENT + test_percent - 1.0) > 1e-6:\n",
    "        raise ValueError(f\"Dataset percents must add up to 1, not {TRAIN_PERCENT + DEV_PERCENT + test_percent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a6ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(df):\n",
    "    \"\"\"Resample dataframe to consistent sampling rate\"\"\"\n",
    "    print(\"RESAMPLE has not been added yet so you need to implement the function\")\n",
    "    raise RuntimeError(\"The resample function has not been implemented\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d72c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_for_gaps(df):\n",
    "    \"\"\"Split dataframe on time gaps larger than threshold\"\"\"\n",
    "    gap_threshold_ns = THRESHOLD_FOR_GAP * 60 * 1_000_000_000\n",
    "    df = df.sort_values('ns_since_reboot').reset_index(drop=True)\n",
    "    time_diffs = df['ns_since_reboot'].diff()\n",
    "    gap_indices = time_diffs[time_diffs > gap_threshold_ns].index\n",
    "    \n",
    "    if len(gap_indices) == 0:\n",
    "        return [df]\n",
    "    \n",
    "    # Split into segments\n",
    "    segments = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    for gap_idx in gap_indices:\n",
    "        if start_idx < gap_idx:\n",
    "            segment = df.iloc[start_idx:gap_idx].copy()\n",
    "            if not segment.empty:\n",
    "                segments.append(segment)\n",
    "        start_idx = gap_idx\n",
    "    \n",
    "    # Add final segment\n",
    "    if start_idx < len(df):\n",
    "        final_segment = df.iloc[start_idx:].copy()\n",
    "        if not final_segment.empty:\n",
    "            segments.append(final_segment)\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a103bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_df(df, type):\n",
    "    if set(['ns_since_reboot', 'x', 'y', 'z']).issubset(set(df.columns)):\n",
    "        df = df.rename(columns={\"x\": f\"x_{type}\", \"y\": f\"y_{type}\", \"z\": f\"z_{type}\"})\n",
    "    elif set(['ns_since_reboot', f'{type}_x', f'{type}_y', f'{type}_z']).issubset(set(df.columns)):\n",
    "        df = df.rename(columns={f\"{type}_x\": f\"x_{type}\", f\"{type}_y\": f\"y_{type}\", f\"{type}_z\": f\"z_{type}\"})\n",
    "    else:   \n",
    "        # rename columns to avoid conflicts\n",
    "        raise ValueError(f\"Warning coloumn names are: {df.columns} but expected to be: ['ns_since_reboot', 'x', 'y', 'z']\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c01db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(session, project_path: str) -> pd.DataFrame:\n",
    "    \"\"\"combine accelerometer and gyroscope data for a session\"\"\"\n",
    "    data_path = os.path.join(project_path, session['session_name'])\n",
    "    \n",
    "    try:\n",
    "        accl = pd.read_csv(os.path.join(data_path, 'accelerometer_data.csv'))\n",
    "        if USE_GYRO:\n",
    "            gyro = pd.read_csv(os.path.join(data_path, 'gyroscope_data.csv'))\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Warning: Could not find data files for session {session['session_name']}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    accl = rename_df(accl, type='accel')\n",
    "\n",
    "    # ensure data types are correct\n",
    "    for col in ['ns_since_reboot', 'x_accel', 'y_accel', 'z_accel']:\n",
    "        accl[col] = accl[col].astype(float)\n",
    "    \n",
    "    if USE_GYRO:\n",
    "        gyro = rename_df(gyro, type='gyro')\n",
    "        \n",
    "        # combine accelerometer and gyroscope data\n",
    "        combined = pd.merge(accl, gyro, on='ns_since_reboot', how='inner')\n",
    "        column_order = ['ns_since_reboot', 'x_accel', 'y_accel', 'z_accel', 'x_gyro', 'y_gyro', 'z_gyro']\n",
    "    else:\n",
    "        # use only accelerometer data\n",
    "        combined = accl\n",
    "        column_order = ['ns_since_reboot', 'x_accel', 'y_accel', 'z_accel']\n",
    "    \n",
    "    # reorder columns\n",
    "    combined = combined[column_order]\n",
    "    \n",
    "    return combined.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd0bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_labels_to_df(df, session) -> pd.DataFrame:\n",
    "    \"\"\"Add labels to dataframe based on bout annotations\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    bout_starts = []\n",
    "    bout_ends = []\n",
    "\n",
    "    for bout in session.get('bouts', []):\n",
    "        if bout.get('label') in LABEL:\n",
    "            bout_starts.append(bout['start_time'])\n",
    "            bout_ends.append(bout['end_time'])\n",
    "\n",
    "    df['label'] = 0\n",
    "\n",
    "    for start, stop in zip(bout_starts, bout_ends):\n",
    "        mask = (df['ns_since_reboot'] >= start) & (df['ns_since_reboot'] <= stop)\n",
    "        df.loc[mask, 'label'] = LABEL_VALUE\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b44ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(df):\n",
    "    \"\"\"create sliding windows from dataframe\"\"\"\n",
    "    if len(df) < WINDOW_SIZE:\n",
    "        print(f\"Warning: DataFrame too small ({len(df)} < {WINDOW_SIZE}), skipping\")\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    if USE_GYRO:\n",
    "        feature_cols = ['x_accel', 'y_accel', 'z_accel', 'x_gyro', 'y_gyro', 'z_gyro']\n",
    "    else:\n",
    "        feature_cols = ['x_accel', 'y_accel', 'z_accel']\n",
    "        \n",
    "    X_data = df[feature_cols].values\n",
    "    y_data = df['label'].values\n",
    "    \n",
    "    windows_X = []\n",
    "    windows_y = []\n",
    "    \n",
    "    for i in range(0, len(df) - WINDOW_SIZE + 1, STEP_SIZE):\n",
    "        window_X = X_data[i:i + WINDOW_SIZE]\n",
    "        window_y = y_data[i:i + WINDOW_SIZE]\n",
    "        \n",
    "        windows_X.append(window_X)\n",
    "        windows_y.append(window_y)\n",
    "    \n",
    "    return np.array(windows_X), np.array(windows_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a518d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_negative_windows(X, y):\n",
    "    \"\"\"Sample negative windows based on configured percentage\"\"\"\n",
    "    if PERCENT_OF_NEGATIVE_WINDOWS_TO_SAMPLE >= 1.0:\n",
    "        return X, y\n",
    "    \n",
    "    # Find windows with and without labels\n",
    "    has_label = np.any(y > 0, axis=1)\n",
    "    print(f'Positive samples: {np.where(has_label)[0].shape} : Negative Samples  {np.where(~has_label)[0].shape}')\n",
    "    positive_indices = np.where(has_label)[0]\n",
    "    negative_indices = np.where(~has_label)[0]\n",
    "    \n",
    "    # Sample negative windows\n",
    "    num_negative_to_keep = int(len(negative_indices) * PERCENT_OF_NEGATIVE_WINDOWS_TO_SAMPLE)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    sampled_negative_indices = np.random.choice(negative_indices, size=num_negative_to_keep, replace=False)\n",
    "    \n",
    "    # Combine positive and sampled negative windows\n",
    "    keep_indices = np.concatenate([positive_indices, sampled_negative_indices])\n",
    "    keep_indices = np.sort(keep_indices)\n",
    "    \n",
    "    return X[keep_indices], y[keep_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0806e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_session(session, project_path):\n",
    "    \"\"\"Process a single session and return windowed data\"\"\"\n",
    "    df = combine(session, project_path)\n",
    "\n",
    "    if df.empty:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # Check for gaps and split if necessary\n",
    "    segments = check_for_gaps(df)\n",
    "    \n",
    "    all_windows_X = []\n",
    "    all_windows_y = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        if RESAMPLE:\n",
    "            segment = resample(segment)\n",
    "        \n",
    "        segment = apply_labels_to_df(segment, session)\n",
    "        windows_X, windows_y = create_windows(segment)\n",
    "        \n",
    "        if len(windows_X) > 0:\n",
    "            all_windows_X.append(windows_X)\n",
    "            all_windows_y.append(windows_y)\n",
    "    \n",
    "    if not all_windows_X:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # Concatenate all segments\n",
    "    combined_X = np.concatenate(all_windows_X, axis=0)\n",
    "    combined_y = np.concatenate(all_windows_y, axis=0)\n",
    "    \n",
    "    return combined_X, combined_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0033b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_participant(participant):\n",
    "    \"\"\"Process all sessions for a participant\"\"\"\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    \n",
    "    for session in participant.get('sessions', []):\n",
    "        X, y = process_session(session, participant['project_path'])\n",
    "        \n",
    "        if len(X) > 0:\n",
    "            all_X.append(X)\n",
    "            all_y.append(y)\n",
    "    \n",
    "    if not all_X:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # Concatenate all sessions\n",
    "    participant_X = np.concatenate(all_X, axis=0)\n",
    "    participant_y = np.concatenate(all_y, axis=0)\n",
    "    \n",
    "    return participant_X, participant_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed62c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_dataset(ids, labels_data):\n",
    "    \"\"\"Create dataset from participant ids\"\"\"\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    \n",
    "    for project in labels_data['projects']:\n",
    "        participant_id = project['participant']['participant_id']\n",
    "        \n",
    "        if participant_id not in ids:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing participant {participant_id} in project: {project['project_name']}\")\n",
    "        X, y = process_participant(project)\n",
    "        \n",
    "        if len(X) > 0:\n",
    "            all_X.append(X)\n",
    "            all_y.append(y)\n",
    "    \n",
    "    if not all_X:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # Concatenate all participants\n",
    "    dataset_X = np.concatenate(all_X, axis=0)\n",
    "    dataset_y = np.concatenate(all_y, axis=0)\n",
    "    \n",
    "    # Filter negative windows\n",
    "    dataset_X, dataset_y = filter_negative_windows(dataset_X, dataset_y)\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    indices = np.random.permutation(len(dataset_X))\n",
    "    dataset_X = dataset_X[indices]\n",
    "    dataset_y = dataset_y[indices]\n",
    "    \n",
    "    print(f\"Dataset created with {len(dataset_X):,} windows\")\n",
    "    \n",
    "    return dataset_X, dataset_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dbc368",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_dataset(X: np.ndarray, y: np.ndarray, name: str):\n",
    "    \"\"\"Save X and y tensors in a .pt file with the name as name.pt\"\"\"\n",
    "    if len(X) == 0:\n",
    "        print(f\"Warning: No data to save for {name}\")\n",
    "        return\n",
    "    \n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    # Transpose X to have shape (batch_size, features, time_steps)\n",
    "    X_tensor = X_tensor.transpose(1, 2)\n",
    "    \n",
    "    save_path = os.path.join(CURRENT_SAVE_DIR, f\"{name}.pt\")\n",
    "    torch.save((X_tensor, y_tensor), save_path)\n",
    "    print(f\"Saved {name} dataset with shape X: {X_tensor.shape}, y: {y_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6adec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fold(fold_num, train_ids, dev_ids, test_ids, labels_data):\n",
    "    \"\"\"Process a single fold\"\"\"\n",
    "    global CURRENT_SAVE_DIR\n",
    "    \n",
    "    print(f\"\\n\\nProcessing fold {fold_num}/{NUM_FOLDS}...\")\n",
    "    print(f\"TRAIN ids: {train_ids}\")\n",
    "    print(f\"DEV ids: {dev_ids}\")\n",
    "    print(f\"TEST ids: {test_ids}\")\n",
    "    \n",
    "    # Set up fold-specific save directory\n",
    "    CURRENT_SAVE_DIR = os.path.join(KFOLD_BASE_DIR, f\"fold-{fold_num}\", \"data\")\n",
    "    os.makedirs(CURRENT_SAVE_DIR, exist_ok=True)\n",
    "    \n",
    "    # Save fold configuration\n",
    "    save_config(fold_num, train_ids, dev_ids, test_ids)\n",
    "    \n",
    "    # Create and save datasets\n",
    "    print(\"Creating training dataset...\")\n",
    "    train_X, train_y = make_dataset(train_ids, labels_data)\n",
    "    save_dataset(train_X, train_y, \"train\")\n",
    "    \n",
    "    print(\"Creating development dataset...\")\n",
    "    dev_X, dev_y = make_dataset(dev_ids, labels_data)\n",
    "    save_dataset(dev_X, dev_y, \"dev\")\n",
    "    \n",
    "    print(\"Creating test dataset...\")\n",
    "    test_X, test_y = make_dataset(test_ids, labels_data)\n",
    "    save_dataset(test_X, test_y, \"test\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330512e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main execution function\"\"\"\n",
    "print(\"Starting K-Fold Cross-Validation Data Preparation\")\n",
    "print(f\"Participants: {PARTICIPANT_IDS}\")\n",
    "print(f\"Number of folds: {NUM_FOLDS}\")\n",
    "print(f\"Base directory: {KFOLD_BASE_DIR}\")\n",
    "print()\n",
    "\n",
    "# Validate configuration and directory structure\n",
    "validate_parameters()\n",
    "validate_kfold_structure()\n",
    "validate_splits()\n",
    "\n",
    "# Generate k-fold splits\n",
    "print(\"Generating k-fold splits...\")\n",
    "fold_splits = generate_kfold_splits()\n",
    "\n",
    "# Load labels data\n",
    "print(f\"Loading labels from: {LABELS_PATH}\")\n",
    "with open(LABELS_PATH, 'r') as f:\n",
    "    labels_data = json.load(f)\n",
    "\n",
    "# Process each fold\n",
    "for split in fold_splits:\n",
    "    process_fold(\n",
    "        split['fold'],\n",
    "        split['train_ids'],\n",
    "        split['dev_ids'],\n",
    "        split['test_ids'],\n",
    "        labels_data\n",
    "    )\n",
    "\n",
    "print(\"K-Fold cross-validation data preparation completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
