{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34372491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import toml\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de5ff210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEF CONSTATS\n",
    "\n",
    "LABELS_PATH = \"/home/kuba/projects/puff/paper00/experiments/01/data/smoking_labels_export_2025-07-15_22.json\"\n",
    "TRAIN_PERCENT = 0.8\n",
    "DEV_PERCENT = 0.2\n",
    "TEST_PERCENT = (1 - TRAIN_PERCENT - DEV_PERCENT)\n",
    "RANDOM_SEED = 70\n",
    "USE_GYRO = False\n",
    "LABEL = {\"puff\", \"puffs\"}\n",
    "LABEL_VALUE = 1 #what to place in the y vector \n",
    "RESAMPLE = False\n",
    "PERCENT_OF_NEGATIVE_WINDOWS_TO_SAMPLE = 0.5 #from all windows that don't contain a label what percent to sample\n",
    "THRESHOLD_FOR_GAP = 30 #min\n",
    "SAVE_DIR = '/home/kuba/projects/puff/paper00/experiments/01/data'\n",
    "WINDOW_SIZE = 1024\n",
    "STEP_SIZE = WINDOW_SIZE \n",
    "\n",
    "\n",
    "train_sessions = []\n",
    "dev_sessions = []\n",
    "test_sessions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daa3fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_config():\n",
    "    \"\"\"save experiment configuration to toml file\"\"\"\n",
    "    config = {\n",
    "        \"paths\": {\n",
    "            \"labels_path\": LABELS_PATH,\n",
    "            \"save_dir\": SAVE_DIR,\n",
    "        },\n",
    "        \"experiment\": {\n",
    "            \"label\": LABEL,\n",
    "            \"resample\": RESAMPLE,\n",
    "            \"random_seed\": RANDOM_SEED,\n",
    "            \"window_size\": WINDOW_SIZE,\n",
    "            \"step_size\": STEP_SIZE,\n",
    "            \"percent_negative_windows\": PERCENT_OF_NEGATIVE_WINDOWS_TO_SAMPLE,\n",
    "            \"threshold_gap_minutes\": THRESHOLD_FOR_GAP,\n",
    "            \"use_gyro\": USE_GYRO\n",
    "        },\n",
    "        \"split\": {\n",
    "            \"train_percent\": TRAIN_PERCENT,\n",
    "            \"dev_percent\": DEV_PERCENT,\n",
    "            \"test_percent\": TEST_PERCENT,\n",
    "        },\n",
    "        \"splits\": {\n",
    "            \"train_sessions\": train_sessions,\n",
    "            \"dev_essions\": dev_sessions,\n",
    "            \"test_sessions\": test_sessions \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    with open(os.path.join(SAVE_DIR, 'data_config.toml'), \"w\") as f:\n",
    "        toml.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b965f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_splits():\n",
    "    \"\"\"makesure  dataset splits add up to 1.0\"\"\"\n",
    "    if abs(TRAIN_PERCENT + DEV_PERCENT + TEST_PERCENT - 1.0) > 1e-6:\n",
    "        raise ValueError(f\"dataset % must add up to 1, not {TRAIN_PERCENT + DEV_PERCENT + TEST_PERCENT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fddc0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(df) :\n",
    "    \"\"\"resample dataframe to consistent sampling rate\"\"\"\n",
    "    print(\"RESAMPLE has not been added yet so you need to impliment the function\")\n",
    "    raise RuntimeError(\"The resample function has not been implimented \")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c1ef76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_gaps(df):\n",
    "    \"\"\"split dataframe on time gaps larger than threshold\"\"\"\n",
    "    gap_threshold_ns = THRESHOLD_FOR_GAP * 60 * 1_000_000_000\n",
    "    df = df.sort_values('ns_since_reboot').reset_index(drop=True)\n",
    "    time_diffs = df['ns_since_reboot'].diff()\n",
    "    gap_indices = time_diffs[time_diffs > gap_threshold_ns].index\n",
    "    \n",
    "    if len(gap_indices) == 0:\n",
    "        return [df]\n",
    "    \n",
    "    # split into segments\n",
    "    segments = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    for gap_idx in gap_indices:\n",
    "        if start_idx < gap_idx:\n",
    "            segment = df.iloc[start_idx:gap_idx].copy()\n",
    "            if not segment.empty:\n",
    "                segments.append(segment)\n",
    "        start_idx = gap_idx\n",
    "    \n",
    "    # add final segment\n",
    "    if start_idx < len(df):\n",
    "        final_segment = df.iloc[start_idx:].copy()\n",
    "        if not final_segment.empty:\n",
    "            segments.append(final_segment)\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1263eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(session, project_path: str) -> pd.DataFrame:\n",
    "    \"\"\"combine accelerometer and gyroscope data for a session\"\"\"\n",
    "    data_path = os.path.join(project_path, session['session_name'])\n",
    "    \n",
    "    try:\n",
    "        accl = pd.read_csv(os.path.join(data_path, 'accelerometer_data.csv'))\n",
    "        if USE_GYRO:\n",
    "            gyro = pd.read_csv(os.path.join(data_path, 'gyroscope_data.csv'))\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Warning: Could not find data files for session {session['session_name']}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # rename columns to avoid conflicts\n",
    "    accl = accl.rename(columns={\"x\": \"x_acc\", \"y\": \"y_acc\", \"z\": \"z_acc\"})\n",
    "    \n",
    "    # ensure data types are correct\n",
    "    for col in ['ns_since_reboot', 'x_acc', 'y_acc', 'z_acc']:\n",
    "        accl[col] = accl[col].astype(float)\n",
    "    \n",
    "    if USE_GYRO:\n",
    "        gyro = gyro.rename(columns={\"x\": \"x_gyro\", \"y\": \"y_gyro\", \"z\": \"z_gyro\"})\n",
    "        for col in ['ns_since_reboot', 'x_gyro', 'y_gyro', 'z_gyro']:\n",
    "            gyro[col] = gyro[col].astype(float)\n",
    "        \n",
    "        # combine accelerometer and gyroscope data\n",
    "        combined = pd.merge(accl, gyro, on='ns_since_reboot', how='inner')\n",
    "        column_order = ['ns_since_reboot', 'x_acc', 'y_acc', 'z_acc', 'x_gyro', 'y_gyro', 'z_gyro']\n",
    "    else:\n",
    "        # use only accelerometer data\n",
    "        combined = accl\n",
    "        column_order = ['ns_since_reboot', 'x_acc', 'y_acc', 'z_acc']\n",
    "    \n",
    "    # reorder columns\n",
    "    combined = combined[column_order]\n",
    "    \n",
    "    return combined.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfb7d9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_labels_to_df(df, session) -> pd.DataFrame:\n",
    "    \"\"\"add labels to dataframe based on bout annotations\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    bout_starts = []\n",
    "    bout_ends = []\n",
    "\n",
    "    for bout in session.get('bouts', []):\n",
    "        if bout.get('label') in LABEL:\n",
    "            bout_starts.append(bout['start_time'])\n",
    "            bout_ends.append(bout['end_time'])\n",
    "\n",
    "    df['label'] = 0\n",
    "\n",
    "    for start, stop in zip(bout_starts, bout_ends):\n",
    "        mask = (df['ns_since_reboot'] >= start) & (df['ns_since_reboot'] <= stop)\n",
    "        df.loc[mask, 'label'] = LABEL_VALUE\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69fd2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(df):\n",
    "    \"\"\"create sliding windows from dataframe\"\"\"\n",
    "    if len(df) < WINDOW_SIZE:\n",
    "        print(f\"Warning: DataFrame too small ({len(df)} < {WINDOW_SIZE}), skipping\")\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    if USE_GYRO:\n",
    "        feature_cols = ['x_acc', 'y_acc', 'z_acc', 'x_gyro', 'y_gyro', 'z_gyro']\n",
    "    else:\n",
    "        feature_cols = ['x_acc', 'y_acc', 'z_acc']\n",
    "        \n",
    "    X_data = df[feature_cols].values\n",
    "    y_data = df['label'].values\n",
    "    \n",
    "    windows_X = []\n",
    "    windows_y = []\n",
    "    \n",
    "    for i in range(0, len(df) - WINDOW_SIZE + 1, STEP_SIZE):\n",
    "        window_X = X_data[i:i + WINDOW_SIZE]\n",
    "        window_y = y_data[i:i + WINDOW_SIZE]\n",
    "        \n",
    "        windows_X.append(window_X)\n",
    "        windows_y.append(window_y)\n",
    "    \n",
    "    return np.array(windows_X), np.array(windows_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36314f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_negative_windows(X, y) :\n",
    "    \"\"\"sample negative windows based on configured percentage\"\"\"\n",
    "    if PERCENT_OF_NEGATIVE_WINDOWS_TO_SAMPLE >= 1.0:\n",
    "        return X, y\n",
    "    \n",
    "    # find windows with and without labels\n",
    "    has_label = np.any(y > 0, axis=1)\n",
    "    print(f'Positive samples: {np.where(has_label)[0].shape} : Negative Samples  {np.where(~has_label)[0].shape}')\n",
    "    positive_indices = np.where(has_label)[0]\n",
    "    negative_indices = np.where(~has_label)[0]\n",
    "    \n",
    "    # sample negative windows\n",
    "    num_negative_to_keep = int(len(negative_indices) * PERCENT_OF_NEGATIVE_WINDOWS_TO_SAMPLE)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    sampled_negative_indices = np.random.choice(negative_indices, size=num_negative_to_keep, replace=False)\n",
    "    \n",
    "    # combine positive and sampled negative windows\n",
    "    keep_indices = np.concatenate([positive_indices, sampled_negative_indices])\n",
    "    keep_indices = np.sort(keep_indices)\n",
    "    \n",
    "    return X[keep_indices], y[keep_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8a905b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_session(session, project_path) :\n",
    "    \"\"\"process a single session and return windowed data\"\"\"\n",
    "    df = combine(session, project_path)\n",
    "\n",
    "    # sample_interval = df['ns_since_reboot'].diff().median() * 1e-9\n",
    "    # sample_rate = 1 / sample_interval\n",
    "    # print(f\"Sample rate: {sample_rate} Hz\")\n",
    "\n",
    "    if df.empty:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # check for gaps and split if necessary\n",
    "    segments = check_for_gaps(df)\n",
    "    \n",
    "    all_windows_X = []\n",
    "    all_windows_y = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        if RESAMPLE:\n",
    "            segment = resample(segment)\n",
    "        \n",
    "        segment = apply_labels_to_df(segment, session)\n",
    "        windows_X, windows_y = create_windows(segment)\n",
    "        \n",
    "        if len(windows_X) > 0:\n",
    "            all_windows_X.append(windows_X)\n",
    "            all_windows_y.append(windows_y)\n",
    "    \n",
    "    if not all_windows_X:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # concatenate all segments\n",
    "    combined_X = np.concatenate(all_windows_X, axis=0)\n",
    "    combined_y = np.concatenate(all_windows_y, axis=0)\n",
    "    \n",
    "    return combined_X, combined_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b36f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(sessions_to_use):\n",
    "    \"\"\"create dataset from participant ids\"\"\"\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    \n",
    "    for session, _, project_path in sessions_to_use:\n",
    "        if session in sessions_to_use:\n",
    "            X, y = process_session(session, project_path)\n",
    "\n",
    "        if len(X) > 0:\n",
    "            all_X.append(X)\n",
    "            all_y.append(y)\n",
    "    \n",
    "    if not all_X:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # concatenate all participants\n",
    "    dataset_X = np.concatenate(all_X, axis=0)\n",
    "    dataset_y = np.concatenate(all_y, axis=0)\n",
    "    \n",
    "    # filter negative windows\n",
    "    dataset_X, dataset_y = filter_negative_windows(dataset_X, dataset_y)\n",
    "    \n",
    "    # shuffle the dataset\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    indices = np.random.permutation(len(dataset_X))\n",
    "    dataset_X = dataset_X[indices]\n",
    "    dataset_y = dataset_y[indices]\n",
    "    \n",
    "    print(f\"Dataset created with {len(dataset_X):,} windows\")\n",
    "    \n",
    "    return dataset_X, dataset_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a7bf842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sessions(all_sessions):\n",
    "    all_sessions = np.array(all_sessions)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    random_perm = np.random.permutation(len(all_sessions))\n",
    "\n",
    "    train_size = int(len(random_perm) * TRAIN_PERCENT)\n",
    "    dev_size = int(len(random_perm) * DEV_PERCENT)\n",
    "\n",
    "    train_idxs = random_perm[:train_size]\n",
    "    dev_idxs = random_perm[train_size:train_size + dev_size]\n",
    "    test_idxs = random_perm[train_size + dev_size:]\n",
    "    \n",
    "    print(f'TRAIN size: {len(train_idxs)}')\n",
    "    print(f'DEV size: {len(dev_idxs)}')\n",
    "    print(f'TEST size: {len(test_idxs)}')\n",
    "    train, dev, test = all_sessions[train_idxs], all_sessions[dev_idxs], all_sessions[test_idxs]\n",
    "\n",
    "\n",
    "    def add_session_details_to_list(sessions, maping_list):\n",
    "        try:\n",
    "            for session, project_info, _ in sessions:\n",
    "                session_meta_data  = f\"{project_info}_{session['session_name']}\"\n",
    "                maping_list.append(session_meta_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in processing the sessin details: {e}\")\n",
    "\n",
    "    add_session_details_to_list(train, train_sessions)\n",
    "    add_session_details_to_list(dev, dev_sessions)\n",
    "    add_session_details_to_list(test, test_sessions)\n",
    "\n",
    "    return train, dev, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4763a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sessions(labels_data):\n",
    "    all_sessions = [] # list of tuples with session ifo and project id\n",
    "    for project in labels_data['projects']:\n",
    "        participant_code_and_id = f'{project['project_name']}_{project['participant']['participant_code']}_{project['participant']['participant_id']}'\n",
    "        for session in project['sessions']:\n",
    "            # if session['bout_count'] > 0:\n",
    "            lables_present = set()\n",
    "            for bout in session['bouts']:\n",
    "                lables_present.add(bout['label'])\n",
    "\n",
    "            if LABEL.intersection(lables_present):\n",
    "                all_sessions.append((session, participant_code_and_id, project['project_path']))\n",
    "\n",
    "    return all_sessions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cabfc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(X: np.ndarray, y: np.ndarray, name: str):\n",
    "    \"\"\"save X and y tensors in a .pt file with the name as name.pt\"\"\"\n",
    "    if len(X) == 0:\n",
    "        print(f\"Warning: No data to save for {name}\")\n",
    "        return\n",
    "    \n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    # transpose X to have shape (batch_size, features, time_steps)\n",
    "    X_tensor = X_tensor.transpose(1, 2)\n",
    "    \n",
    "    save_path = os.path.join(SAVE_DIR, f\"{name}.pt\")\n",
    "    torch.save((X_tensor, y_tensor), save_path)\n",
    "    print(f\"Saved {name} dataset with shape X: {X_tensor.shape}, y: {y_tensor.shape}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccd87281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN size: 43\n",
      "DEV size: 10\n",
      "TEST size: 1\n",
      "Creating training dataset...\n",
      "Warning: Could not find data files for session 2025-05-15_13_14_47.5: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-15_13_14_47.5/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-14_13_45_24.3: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-14_13_45_24.3/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-19_05_30_05.3: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-19_05_30_05.3/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-04-26_05_29_55: [Errno 2] No such file or directory: '/var/lib/delta/data/P005 Phase 3_P005_20250707_183957/2025-04-26_05_29_55/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-16_05_30_00.2: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-16_05_30_00.2/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2024-12-20_05_12_36: [Errno 2] No such file or directory: '/var/lib/delta/data/Alsaad Phase 3_Alsaad_20250702_180849/2024-12-20_05_12_36/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-14_13_45_24.4: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-14_13_45_24.4/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-20_05_30_21.3: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-20_05_30_21.3/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-13_00_18_06.1: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-13_00_18_06.1/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-17_09_19_41.5: [Errno 2] No such file or directory: '/var/lib/delta/data/Asfik Phase 3_ASFIK_20250623_165048/2025-05-17_09_19_41.5/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-15_05_30_38.2: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-15_05_30_38.2/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-12_12_51_29.5: [Errno 2] No such file or directory: '/var/lib/delta/data/Asfik Phase 3_ASFIK_20250623_165048/2025-05-12_12_51_29.5/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-14_13_45_24.2: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-14_13_45_24.2/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-12_12_51_29.2: [Errno 2] No such file or directory: '/var/lib/delta/data/Asfik Phase 3_ASFIK_20250623_165048/2025-05-12_12_51_29.2/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-12_12_51_29.3: [Errno 2] No such file or directory: '/var/lib/delta/data/Asfik Phase 3_ASFIK_20250623_165048/2025-05-12_12_51_29.3/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-20_05_30_21.2: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-20_05_30_21.2/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-04-29_13_17_14: [Errno 2] No such file or directory: '/var/lib/delta/data/P005 Phase 3_P005_20250707_183957/2025-04-29_13_17_14/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-12_12_51_29.4: [Errno 2] No such file or directory: '/var/lib/delta/data/Asfik Phase 3_ASFIK_20250623_165048/2025-05-12_12_51_29.4/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-12_15_14_59.2: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-12_15_14_59.2/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-16_14_04_12: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-16_14_04_12/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-13_00_18_06.2: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-13_00_18_06.2/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-14_05_30_02.2: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-14_05_30_02.2/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-19_05_30_05.2: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-19_05_30_05.2/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-17_09_19_41.2: [Errno 2] No such file or directory: '/var/lib/delta/data/Asfik Phase 3_ASFIK_20250623_165048/2025-05-17_09_19_41.2/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-17_10_04_25.2: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-17_10_04_25.2/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-13_23_13_50: [Errno 2] No such file or directory: '/var/lib/delta/data/P001 Phase 3_P001_20250707_182055/2025-05-13_23_13_50/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-19_05_30_05.4: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-19_05_30_05.4/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2024-12-20_15_18_00.2: [Errno 2] No such file or directory: '/var/lib/delta/data/Alsaad Phase 3_Alsaad_20250702_180849/2024-12-20_15_18_00.2/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-12_12_51_29.1: [Errno 2] No such file or directory: '/var/lib/delta/data/Asfik Phase 3_ASFIK_20250623_165048/2025-05-12_12_51_29.1/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-17_09_19_41.4: [Errno 2] No such file or directory: '/var/lib/delta/data/Asfik Phase 3_ASFIK_20250623_165048/2025-05-17_09_19_41.4/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-19_05_30_05.5: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-19_05_30_05.5/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-15_13_14_47.2: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-15_13_14_47.2/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-17_09_19_41.3: [Errno 2] No such file or directory: '/var/lib/delta/data/Asfik Phase 3_ASFIK_20250623_165048/2025-05-17_09_19_41.3/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-16_15_35_21: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-16_15_35_21/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-04-25_11_34_20: [Errno 2] No such file or directory: '/var/lib/delta/data/P005 Phase 3_P005_20250707_183957/2025-04-25_11_34_20/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-14_13_08_10: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-14_13_08_10/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-17_09_19_41.6: [Errno 2] No such file or directory: '/var/lib/delta/data/Asfik Phase 3_ASFIK_20250623_165048/2025-05-17_09_19_41.6/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-19_16_43_28: [Errno 2] No such file or directory: '/var/lib/delta/data/Asfik Phase 3_ASFIK_20250623_165048/2025-05-19_16_43_28/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-20_10_54_16: [Errno 2] No such file or directory: '/var/lib/delta/data/Asfik Phase 3_ASFIK_20250623_165048/2025-05-20_10_54_16/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-01-01_10_18_52.2: [Errno 2] No such file or directory: '/var/lib/delta/data/P001 Phase 3_P001_20250707_182055/2025-01-01_10_18_52.2/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-04-24_01_11_03: [Errno 2] No such file or directory: '/var/lib/delta/data/P005 Phase 3_P005_20250707_183957/2025-04-24_01_11_03/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-15_13_14_47.4: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-15_13_14_47.4/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-13_13_29_52.4: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-13_13_29_52.4/accelerometer_data.csv'\n",
      "Warning: No data to save for train\n",
      "Creating development dataset...\n",
      "Warning: Could not find data files for session 2025-05-15_13_14_47.3: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-15_13_14_47.3/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-12_12_51_29.6: [Errno 2] No such file or directory: '/var/lib/delta/data/Asfik Phase 3_ASFIK_20250623_165048/2025-05-12_12_51_29.6/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-18_19_51_54: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-18_19_51_54/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-13_00_18_06.3: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-13_00_18_06.3/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-04-23_12_53_54: [Errno 2] No such file or directory: '/var/lib/delta/data/P005 Phase 3_P005_20250707_183957/2025-04-23_12_53_54/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-13_13_29_52.2: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-13_13_29_52.2/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2024-06-11_09_28_01: [Errno 2] No such file or directory: '/var/lib/delta/data/P001 Phase 2_P001_20250708_174621/2024-06-11_09_28_01/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-13_13_29_52.5: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-13_13_29_52.5/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-04-29_05_30_02: [Errno 2] No such file or directory: '/var/lib/delta/data/P005 Phase 3_P005_20250707_183957/2025-04-29_05_30_02/accelerometer_data.csv'\n",
      "Warning: Could not find data files for session 2025-05-13_13_29_52.3: [Errno 2] No such file or directory: '/var/lib/delta/data/Tonmoy Phase 3_Tonmoy_20250623_162946/2025-05-13_13_29_52.3/accelerometer_data.csv'\n",
      "Warning: No data to save for dev\n"
     ]
    }
   ],
   "source": [
    "\"\"\"main execution function\"\"\"\n",
    "\n",
    "# validate configuration\n",
    "validate_splits()\n",
    "\n",
    "# save configuration\n",
    "# save_config()\n",
    "\n",
    "# load labels data\n",
    "with open(LABELS_PATH, 'r') as f:\n",
    "    labels_data = json.load(f)\n",
    "\n",
    "all_sessions = get_all_sessions(labels_data)\n",
    "\n",
    "train_data, dev_data, test_data = split_sessions(all_sessions)\n",
    "\n",
    "# create and save datasets\n",
    "print(\"Creating training dataset...\")\n",
    "train_X, train_y = make_dataset(train_data)\n",
    "save_dataset(train_X, train_y, \"train\")\n",
    "\n",
    "print(\"Creating development dataset...\")\n",
    "dev_X, dev_y = make_dataset(dev_data)\n",
    "save_dataset(dev_X, dev_y, \"dev\")\n",
    "\n",
    "# print(\"Creating test dataset...\")\n",
    "# test_X, test_y = make_dataset(test_data)\n",
    "# save_dataset(test_X, test_y, \"test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
